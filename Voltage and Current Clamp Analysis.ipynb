{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import efel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# define some functions\n",
    "def find_spike_times(voltages, dt, detection_level, min_interval):\n",
    "    spike_times = []\n",
    "    last_spike_time = -min_interval\n",
    "    for i in range(1, len(voltages)):\n",
    "        t = i * dt\n",
    "        if (voltages[i - 1] < detection_level <= voltages[i]) and (t - last_spike_time >= min_interval):\n",
    "            spike_times.append(t)\n",
    "            last_spike_time = t\n",
    "    return spike_times\n",
    "\n",
    "\n",
    "def group_cvs(values, group_size):\n",
    "    cvs = []\n",
    "    if len(values) >= group_size:\n",
    "        for i in range(0, len(values) - group_size, group_size):\n",
    "            mu = np.mean(values[i:i + group_size])\n",
    "            sigma = np.std(values[i:i + group_size])\n",
    "            cvs.append(sigma/mu)\n",
    "    return cvs\n",
    "\n",
    "def segment(values, dx, x_min, x_max):\n",
    "    return values[round(x_min / dx):round(x_max / dx)]\n",
    "\n",
    "def find_slopes(values, dx):\n",
    "    diffs = np.diff(values)\n",
    "    slopes = [0] * len(values)\n",
    "    slopes[0] = diffs[0] / dx\n",
    "    slopes[-1] = diffs[-1] / dx\n",
    "    for i in range(1, len(values) - 1):\n",
    "        slopes[i] = (diffs[i - 1] + diffs[i]) / (2 * dx)\n",
    "    return (slopes)\n",
    "\n",
    "#use neo to import either voltage or current clamp data in the correct, scaled units!\n",
    "def load_neo_file(file_name, **kwargs):\n",
    "    import neo\n",
    "    reader = neo.io.get_io(file_name)\n",
    "    blocks = reader.read(**kwargs)\n",
    "    new_blocks = []\n",
    "    for bl in blocks:\n",
    "        new_segments = []\n",
    "        for seg in bl.segments:\n",
    "            traces = []\n",
    "            count_traces = 0\n",
    "            analogsignals = seg.analogsignals\n",
    "\n",
    "            for sig in analogsignals:\n",
    "                traces.append({})\n",
    "                traces[count_traces]['T'] = sig.times.rescale('ms').magnitude\n",
    "                #need to write an if statement here for conversion\n",
    "                try:\n",
    "                    traces[count_traces]['A'] = sig.rescale('pA').magnitude\n",
    "                except:\n",
    "                    traces[count_traces]['V'] = sig.rescale('mV').magnitude\n",
    "                count_traces += 1\n",
    "            new_segments.append(traces)\n",
    "        #new_blocks.append(efel_segments)\n",
    "    return new_segments\n",
    "\n",
    "def dvdt(path, sweep):\n",
    "    table2 = []\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        #check whether file is in the axgx or axgd format\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=500, stim_end=2500)\n",
    "            for data in traces[sweep]:\n",
    "                times = (data['T']) / 1000\n",
    "                voltages = (data['V'])\n",
    "                times -= times[0]\n",
    "                dt = times[2] - times[1]\n",
    "                detection_level = 0\n",
    "                min_interval = 0.0001\n",
    "                spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                isis = np.diff(spike_times)\n",
    "                time_before = .018\n",
    "                time_after = .015\n",
    "                times_rel = list(np.arange(-time_before, time_after, dt))\n",
    "                spike_voltages = []\n",
    "                for i in range(0, len(spike_times)):\n",
    "                    if time_before < spike_times[i] < times[-1] - time_after:\n",
    "                        spike_voltages.append(\n",
    "                            segment(voltages, dt, spike_times[i] - time_before, spike_times[i] + time_after))\n",
    "                spike_voltage_arrays = [np.array(x) for x in spike_voltages]\n",
    "                mean_spike_voltages = [np.mean(k) for k in zip(*spike_voltage_arrays)]\n",
    "                dvdt_threshold = 20\n",
    "                dvdt = find_slopes(mean_spike_voltages, dt)\n",
    "                i = 1\n",
    "                while dvdt[i] < dvdt_threshold:\n",
    "                    i += 1\n",
    "                v0 = mean_spike_voltages[i - 1]\n",
    "                v1 = mean_spike_voltages[i]\n",
    "                dvdt0 = dvdt[i - 1]\n",
    "                dvdt1 = dvdt[i]\n",
    "                v_threshold = v0 + (v1 - v0) * (dvdt_threshold - dvdt0) / (dvdt1 - dvdt0)\n",
    "                pandas_dvdt = pd.DataFrame(dvdt)\n",
    "                pandas_dvdt.rename(columns={0:filename}, inplace=True) #naming the columns!\n",
    "                pandas_membrane_voltages = pd.DataFrame(mean_spike_voltages)\n",
    "            table2.append(pandas_dvdt)\n",
    "            df_concat = pd.concat(table2, axis=1)\n",
    "\n",
    "            df_concat.to_excel('dvdt' + 'master_file.xlsx', index=False)\n",
    "    return(df_concat)\n",
    "\n",
    "#write some functions for the rest of this stuff\n",
    "def analyze_feature(path, feature):\n",
    "    table2 = []\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        table = pd.DataFrame(columns=[feature])     #create a table that has columns with the name you want\n",
    "        table.name = feature                        #the tables name\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):    #check for the filetype\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=500, stim_end=2500)    #load the trace, and define stim start and stop\n",
    "            for data in traces:    #loop through these guys\n",
    "                #table.rename(columns={feature:filename}, inplace=True) #renaming the columns with the correct file !\n",
    "                feature_values = efel.getFeatureValues(data, [feature], raise_warnings=None)[0]  #this is the feature extraction\n",
    "                if feature_values[feature] is not None:\n",
    "                    # Define the parameters for detection\n",
    "                    efel.api.setThreshold(-10) # Voltage threshold for detection\n",
    "                    efel.api.setDerivativeThreshold(20) # dV/dt threshold for detection\n",
    "                    efel.setIntSetting('strict_stiminterval', True)\n",
    "                    length = len(table)\n",
    "                    table.loc[length, feature] = feature_values[feature][0]\n",
    "\n",
    "                else:\n",
    "                    efel.api.setThreshold(-10) # Voltage threshold for detection\n",
    "                    efel.api.setDerivativeThreshold(20) # dV/dt threshold for detection\n",
    "                    efel.setIntSetting('strict_stiminterval', True)\n",
    "                    length = len(table)\n",
    "                    table.loc[length, feature] = feature_values[feature]\n",
    "\n",
    "            table2.append(table)\n",
    "            df_concat = pd.concat(table2, axis=1)\n",
    "            table.rename(columns={feature:filename}, inplace=True) #renaming the columns with the correct file !\n",
    "            #block of code to combine all of the generated excel workbooks into a single workbook\n",
    "            df_concat.to_excel(feature + 'master_file.xlsx', index=False)\n",
    "    Current_injected = np.linspace(-100.0, 600.0, num=15)\n",
    "    table2 = df_concat.assign(Current_injected=Current_injected)\n",
    "    #lineplot = df_concat.plot()\n",
    "    #sns_lineplot = sns.relplot(data = table2, x = \"Spikecount_stimint\", y = 'Current_injected', kind=\"line\")\n",
    "    return(df_concat)\n",
    "\n",
    "\n",
    "def mean_spike_voltages(path, sweep):\n",
    "    table2 = []\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        #check whether file is in the axgx or axgd format\n",
    "        if filename.endswith(\".axgd\") or filename.endswith(\".axgx\"):\n",
    "            [traces] = efel.io.load_neo_file(filename, stim_start=500, stim_end=2500)\n",
    "            for data in traces[sweep]:\n",
    "                times = (data['T'])/1000\n",
    "                voltages = (data['V'])\n",
    "                times -= times[0]\n",
    "                dt = times[2] - times[1]\n",
    "                detection_level = 0\n",
    "                min_interval = 0.0001\n",
    "                spike_times = find_spike_times(voltages, dt, detection_level, min_interval)\n",
    "                isis = np.diff(spike_times)\n",
    "                time_before = .018\n",
    "                time_after = .015\n",
    "                times_rel = list(np.arange(-time_before, time_after, dt))\n",
    "                spike_voltages = []\n",
    "                for i in range(0, len(spike_times)):\n",
    "                    if time_before < spike_times[i] < times[-1] - time_after:\n",
    "                        spike_voltages.append(segment(voltages, dt, spike_times[i] - time_before, spike_times[i] + time_after))\n",
    "                spike_voltage_arrays = [np.array(x) for x in spike_voltages]\n",
    "                mean_spike_voltages = [np.mean(k) for k in zip(*spike_voltage_arrays)]\n",
    "                dvdt_threshold = 20\n",
    "                dvdt = find_slopes(mean_spike_voltages, dt)\n",
    "                i = 1\n",
    "                while dvdt[i] < dvdt_threshold:\n",
    "                    i += 1\n",
    "                v0 = mean_spike_voltages[i - 1]\n",
    "                v1 = mean_spike_voltages[i]\n",
    "                dvdt0 = dvdt[i - 1]\n",
    "                dvdt1 = dvdt[i]\n",
    "                v_threshold = v0 + (v1 - v0) * (dvdt_threshold - dvdt0) / (dvdt1 - dvdt0)\n",
    "                pandas_dvdt = pd.DataFrame(dvdt)\n",
    "                pandas_membrane_voltages = pd.DataFrame(mean_spike_voltages)\n",
    "                pandas_membrane_voltages.rename(columns={0:filename}, inplace=True)\n",
    "\n",
    "            table2.append(pandas_membrane_voltages)\n",
    "            df_concat = pd.concat(table2, axis=1)\n",
    "            pandas_times_rel = pd.DataFrame(times_rel)\n",
    "            df_concat.to_excel('mean_spike_voltages' + 'master_file.xlsx', index=False)\n",
    "            pandas_times_rel.to_excel('times_rel_for_VTA.xlsx', index=False)\n",
    "    return(df_concat, pandas_times_rel)\n",
    "\n",
    "def A_current(path):\n",
    "    os.chdir(path)\n",
    "    table2 = []\n",
    "    table3 = []\n",
    "    table4 = []\n",
    "    for file_name in os.listdir():\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            file = load_neo_file(file_name)\n",
    "            def monoExp(x, m, t, b):\n",
    "                return m * np.exp(-t * x) + b\n",
    "\n",
    "            for traces in file:\n",
    "                for data in traces:\n",
    "                    #this is for the first part of the A-current\n",
    "                    times1 = data['T'][10050:20000]\n",
    "                    amps1 = data['A'][10050:20000]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][:3000])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "                    max_amp1 = np.amax(amps1)  #find the maximum of the baseline subtracted amplitudes\n",
    "                    #working on integration\n",
    "                    flat_amps_1 = np.ndarray.flatten(\n",
    "                        amps1)  #here we have to flatten our array into a single dimension so we can integrate\n",
    "                    integrate1 = np.trapz(flat_amps_1)  #lets integrate this function using the numpy trapezius function\n",
    "                    #this is for the substraction of the next part, so that we can get the substracted information\n",
    "                    times2 = data['T'][\n",
    "                             35050:40000]  #in order to extract a-current, we need to subtract from the inactivated standing current\n",
    "                    amps2 = data['A'][\n",
    "                            35050:40000]  #in order to extract a-current, we need to subtract from the inactivated standing current\n",
    "                    #plt.plot(times2, amps2) #inspect\n",
    "                    amps2 = amps2 - baseline  #find the baseline subtracted values for the inactivated standing current\n",
    "                    max_amp2 = np.amax(amps2)  #find the max amplitude of that current\n",
    "                    flat_amps_2 = np.ndarray.flatten(\n",
    "                        amps2)  #flatten the baseline substracted inactivated portion for integration\n",
    "                    integrate2 = np.trapz(flat_amps_2)  #integrate flattened baseline substracted sweeps\n",
    "                    #now we need to substract that information to get the A current values\n",
    "                    a_current_max_amp = max_amp1 - max_amp2  #this and the next calls are for the actual data we want, here max amp\n",
    "                    a_current_auc = integrate1 - integrate2  #and here AUC\n",
    "                    #from this point foward we are trying to put data together for iteration\n",
    "                    #print('A-Current Max Amp is ', a_current_max_amp, 'pA')\n",
    "                    #print('A-Current AUC is', a_current_auc, 'pA*s')\n",
    "                    #generate a dataframe, must pass a 2d array\n",
    "                    a_current_max_amp_array = np.array(a_current_max_amp, ndmin=2)\n",
    "                    max_amp_df = pd.DataFrame(a_current_max_amp_array)\n",
    "                    max_amp_df['file_name'] = file_name  #adding a column to add the filename\n",
    "\n",
    "                    #generate a dataframe, must mass a 2d array\n",
    "                    a_current_auc_array = np.array(a_current_auc, ndmin=2)\n",
    "                    auc_df = pd.DataFrame(a_current_auc_array)\n",
    "                    auc_df['file_name'] = file_name  #adding a column to add the filename\n",
    "\n",
    "                    #fit a curve using scipy functionality - these params seem to work for a-current\n",
    "                    p0 = [500, .001, 50]  #values near what we expect   #here\n",
    "                    params, cv = scipy.optimize.curve_fit(monoExp, times1, flat_amps_1, p0, bounds=(-np.inf, np.inf), maxfev=50000)  #here\n",
    "                    m, t, b = params  #here\n",
    "                    #m, t = params\n",
    "                    sampleRate = 10_000  #hz\n",
    "                    tauSec = (1 / t) / sampleRate\n",
    "                    #determine quality of fit\n",
    "                    squaredDiffs = np.square(flat_amps_1 - monoExp(times1, m, t, b))  #here\n",
    "                    squaredDiffsFromMean = np.square(flat_amps_1 - np.mean(flat_amps_1))\n",
    "                    rSquared = 1 - np.sum(squaredDiffs) / np.sum(\n",
    "                        squaredDiffsFromMean)  #we want these, but they arent super important to display\n",
    "                    #print(f\"R^2 = {rSquared}\")\n",
    "\n",
    "                    #plot results\n",
    "                    #plt.plot(times1, flat_amps_1, '.', label=\"data\")\n",
    "                    #plt.plot(times1, monoExp(times1, m, t, b), '--', label=\"fitted\")  #here\n",
    "                    #plt.show()\n",
    "                    #plt.title(\"Fitted Expotential Curve\")\n",
    "\n",
    "                    #inspect the params\n",
    "                    #print(f\"Y = {m} * e^(-{t} * x) + {b}\")   #the equations are important\n",
    "                    #print(f\"Tau = {tauSec * 1e6} us\")    #but the tau is the most important\n",
    "                    tau_array = np.array(tauSec * 1e4, ndmin=2)\n",
    "\n",
    "                    tau_df = pd.DataFrame(tau_array)\n",
    "                    tau_df['file_name'] = file_name  #adding a column to add the filename\n",
    "                table2.append(max_amp_df)\n",
    "\n",
    "                table3.append(auc_df)\n",
    "                table4.append(tau_df)\n",
    "\n",
    "        amp_concat = pd.concat(table2, ignore_index=True, axis=0)\n",
    "        amp_concat.rename(columns = {0:'Control Max Amplitude(pA)'}, inplace=True)\n",
    "        #amp_concat['Sweep'] = amp_concat.index%5 + 1\n",
    "        every_4th_sweep_amp = amp_concat[amp_concat.index % 5 == 3]\n",
    "        every_4th_sweep_amp.to_excel('a_current_amp' + '.xlsx', index=False)\n",
    "\n",
    "        auc_concat = pd.concat(table3, ignore_index=True, axis=0)\n",
    "        auc_concat.rename(columns = {0:'Control AUC (pA*s)'}, inplace=True)\n",
    "        #auc_concat['Sweep'] = amp_concat.index%5 + 1\n",
    "        every_4th_sweep_auc = auc_concat[amp_concat.index % 5 == 3]\n",
    "        every_4th_sweep_auc.to_excel('a_current_auc' + '.xlsx', index=False)\n",
    "\n",
    "        tau_concat = pd.concat(table4, ignore_index=True, axis=0)\n",
    "        tau_concat.rename(columns = {0:'Control Tau (ms)'}, inplace=True)\n",
    "        #tau_concat['Sweep'] = amp_concat.index%5 + 1\n",
    "        every_4th_sweep_tau = tau_concat[amp_concat.index % 5 == 3]\n",
    "        every_4th_sweep_tau.to_excel('a_current_tau' + '.xlsx', index=False)\n",
    "    return display(every_4th_sweep_amp), display(every_4th_sweep_auc), display(every_4th_sweep_tau)\n",
    "\n",
    "def sk_analysis(path):\n",
    "\n",
    "    def monoExp(x, m, t, b):\n",
    "        return m * np.exp(-t * x) + b\n",
    "\n",
    "    ahc_append = []\n",
    "    ahc_auc_append = []\n",
    "\n",
    "    ahc_max_amp_append = []\n",
    "    auc_append = []\n",
    "    tau_append = []\n",
    "    ahc_max_amp_df_append = []\n",
    "    os.chdir(path)\n",
    "\n",
    "    for file_name in os.listdir():\n",
    "        tabla = []\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            traces = load_neo_file(file_name)\n",
    "            for sk_data in traces:\n",
    "                for data in sk_data:\n",
    "                    #this is for the first part of the A-current\n",
    "                    times1 = data['T'][322002:324260]  #this is 16.101 to 16.213\n",
    "                    amps1 = data['A'][322002:324260]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][318400:319600])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "                    amps1_df = pd.DataFrame(amps1) #we generated all the amps into a dataframe, check\n",
    "                    flat_times = np.ndarray.flatten(times1) #we have all of the times into a flattened numpy array, check\n",
    "                    tabla.append(amps1_df) #and we appended all of the amps into a dataframe\n",
    "                amps_concat = pd.concat(tabla, axis=1) #this is correct - we put all the amps for a given trace into a dataframe\n",
    "            #now we need to average those dataframes by row\n",
    "            averaged_sk_trace = amps_concat.mean(axis=1)  #woohooo this is the averaged SK trace in a pd.DF, this is what we want to work with from here on out!\n",
    "\n",
    "            #Block of code of AHC Max Amp !\n",
    "            ahc_max_amp = pd.DataFrame.max(averaged_sk_trace)  #max of the whole ahc, not just sk component, This is the value!!\n",
    "            #figure out to how put this in a format that can be read and concatenated\n",
    "            ahc_max_amp_array = np.array(ahc_max_amp, ndmin=2)   #these lines are new\n",
    "            ahc_max_amp_df = pd.DataFrame(ahc_max_amp_array)  #these lines are new - this line of code produces the correct values but does not add them into an appended dataframe. which is obviously a problem. the task is to get these and all the values from the other two measures into a single dataframe so that in the future we may compare across groups within python<3\n",
    "            ahc_max_amp_df['file_name'] = file_name  #adding a column to add the filename\n",
    "            ahc_append.append(ahc_max_amp_df) #Here!\n",
    "            ahc_max_amp_concat_df = pd.concat(ahc_append) #This is the variable for ahc\n",
    "\n",
    "            #Block of code for AHC AUC\n",
    "            averaged_sk_trace_as_np = averaged_sk_trace.to_numpy()\n",
    "            flattened_average_sk_trace = np.ndarray.flatten(averaged_sk_trace_as_np)\n",
    "            ahc_auc = np.trapz(flattened_average_sk_trace)  #this is the auc of the whole ahc\n",
    "            #bit of code to get the ahc auc into readable condition\n",
    "            ahc_auc_array = np.array(ahc_auc, ndmin=2)\n",
    "            ahc_auc_df = pd.DataFrame(ahc_auc_array)  #here we have the auc data in a dataframe\n",
    "            ahc_auc_df['file_name'] = file_name\n",
    "            ahc_auc_append.append(ahc_auc_df)\n",
    "            ahc_auc_concat_df = pd.concat(ahc_auc_append) #this is the variable for auc\n",
    "\n",
    "            #Block of code for kinetics\n",
    "            trace_for_kinetics = flattened_average_sk_trace[30:]\n",
    "            times_for_kinetics = flat_times[30:]\n",
    "            trace_for_kinetics_pd = pd.DataFrame(trace_for_kinetics)\n",
    "            #trace_for_kinetics_pd.to_excel(\"trace_for_kinetics_pd.xlsx\")\n",
    "            times_for_kinetics_pd = pd.DataFrame(times_for_kinetics)\n",
    "            #times_for_kinetics_pd.to_excel(\"times_for_kinetics_pd.xlsx\")\n",
    "\n",
    "            #fit the curve for inactivation tau\n",
    "            p0 = [1.187 * 10 ** 150., .02, +16]  #values near what we expect   #here\n",
    "            params, cv = scipy.optimize.curve_fit(monoExp, times_for_kinetics, trace_for_kinetics, p0,\n",
    "                                                  bounds=(-np.inf, np.inf),\n",
    "                                                  maxfev=100000)  #here  #this fits the training curve with an r-squared of 0.97\n",
    "            m, t, b = params  #here\n",
    "            #m, t = params\n",
    "            sampleRate = 20_000  #hz\n",
    "            tauSec = (1 / t) / sampleRate\n",
    "\n",
    "            #determine quality of fit\n",
    "            squaredDiffs = np.square(trace_for_kinetics - monoExp(times_for_kinetics, m, t, b))  #here\n",
    "            squaredDiffsFromMean = np.square(trace_for_kinetics - np.mean(trace_for_kinetics))\n",
    "            rSquared = 1 - np.sum(squaredDiffs) / np.sum(\n",
    "                squaredDiffsFromMean)  #we want these, but they arent super important to display\n",
    "            #print(f\"R^2 = {rSquared}\")\n",
    "\n",
    "            #plot results\n",
    "            #plt.plot(times_for_kinetics, trace_for_kinetics, '.', label=\"data\")\n",
    "            #plt.plot(times_for_kinetics, monoExp(times_for_kinetics, m, t, b), '--', label=\"fitted\")  #here\n",
    "            plt.show()\n",
    "            #plt.title(\"Fitted Expotential Curve\")\n",
    "\n",
    "            #inspect the params\n",
    "            #print(f\"Y = {m} * e^(-{t} * x) + {b}\")   #the equations are important\n",
    "            #print(f\"Tau = {tauSec * 1e6} us\")    #but the tau is the most important\n",
    "            plt.show()\n",
    "            tau_flat_ms = tauSec * 1e4\n",
    "\n",
    "            #Bit of code to get tau into working order\n",
    "            tau_array = np.array(tauSec * 1e4, ndmin=2)\n",
    "            tau_df = pd.DataFrame(tau_array)\n",
    "            tau_df['file_name'] = file_name\n",
    "            tau_append.append(tau_df)\n",
    "            tau_concat_df = pd.concat(tau_append)   #this is the variable for tau\n",
    "\n",
    "    #lets rename columns and export to excel for each of our metrics\n",
    "    ahc_max_amp_concat_df.rename(columns = {0:'Control AHC Max_Amplitude (pA)'}, inplace=True)\n",
    "    ahc_max_amp_concat_df.to_excel('ahc_max_amp_' + '.xlsx', index=False)\n",
    "\n",
    "    ahc_auc_concat_df.rename(columns = {0:'Control AHC AUC (pA*s)'}, inplace=True)\n",
    "    ahc_auc_concat_df.to_excel('ahc_auc' + '.xlsx', index=False)\n",
    "\n",
    "    tau_concat_df.rename(columns = {0:'Control Decay Tau (ms)'}, inplace=True)\n",
    "    tau_concat_df.to_excel('ahc_tau' + '.xlsx', index=False)\n",
    "\n",
    "    return display(ahc_max_amp_concat_df), display(ahc_auc_concat_df), display(tau_concat_df)\n",
    "\n",
    "def generalized_v_clamp_analysis(path, sampling_rate_hz, analysis_time_start_ms, analysis_time_end_ms, baseline_start_ms, baseline_end_ms, time_from_analysis_start_time_for_decay_tau):\n",
    "    os.chdir(path)\n",
    "    ahc_append = []\n",
    "    ahc_auc_append = []\n",
    "    tau_append = []\n",
    "\n",
    "    def monoExp(x, m, t, b):\n",
    "        return m * np.exp(-t * x) + b\n",
    "    #here we need to convert sampling rate to time in ms, and as integers, otherwise we cannot slice with them\n",
    "    analysis_time_start1 = int(sampling_rate_hz * analysis_time_start_ms)\n",
    "    analysis_time_end1 = int(sampling_rate_hz * analysis_time_end_ms)\n",
    "    baseline_start1 = int(sampling_rate_hz * baseline_start_ms)\n",
    "    baseline_end1 = int(sampling_rate_hz * baseline_end_ms)\n",
    "    time_from_analysis_start_time_for_decay_tau1 = int(sampling_rate_hz * time_from_analysis_start_time_for_decay_tau)\n",
    "    for file_name in os.listdir():\n",
    "        tabla = []\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            traces = load_neo_file(file_name)\n",
    "            for sk_data in traces:\n",
    "                for data in sk_data:\n",
    "                    #this is for the first part of the A-current\n",
    "                    #time1_slice = slice(analysis_time_start1:analysis_time_end1)\n",
    "                    times1 = data['T'][analysis_time_start1:analysis_time_end1]  #this is 16.101 to 16.213\n",
    "                    amps1 = data['A'][analysis_time_start1:analysis_time_end1]\n",
    "                    #plt.plot(times, amps) #inspect\n",
    "                    baseline = np.mean(data['A'][baseline_start1:baseline_end1])  #define a baseline period from which to substract from\n",
    "                    amps1 = amps1 - baseline  #subtract the baseline from the amplitudes of the first pulse\n",
    "                    amps1_df = pd.DataFrame(amps1)\n",
    "                    flat_times = np.ndarray.flatten(times1)\n",
    "                    tabla.append(amps1_df)\n",
    "                amps_concat = pd.concat(tabla, axis=1)\n",
    "            averaged_sk_trace = amps_concat.mean(axis=1)  #woohooo this is the averaged SK trace in a pd.DF\n",
    "\n",
    "            #Block of code for AHC Max Amplitude in pA\n",
    "            ahc_max_amp = pd.DataFrame.max(averaged_sk_trace)  #max of the whole ahc, not just sk component\n",
    "            ahc_max_amp_array = np.array(ahc_max_amp, ndmin=2)   #these lines are new\n",
    "            ahc_max_amp_df = pd.DataFrame(ahc_max_amp_array)  #these lines are new\n",
    "            ahc_max_amp_df['file_name'] = file_name\n",
    "            ahc_append.append(ahc_max_amp_df) #Here!\n",
    "            ahc_max_amp_concat_df = pd.concat(ahc_append)\n",
    "\n",
    "            #Block of code for AUC\n",
    "            averaged_sk_trace_as_np = averaged_sk_trace.to_numpy()\n",
    "            flattened_average_sk_trace = np.ndarray.flatten(averaged_sk_trace_as_np)\n",
    "            ahc_auc = np.trapz(flattened_average_sk_trace)  #this is the auc of the whole ahc\n",
    "            #bit of code to get the ahc auc into readable condition\n",
    "            ahc_auc_array = np.array(ahc_auc, ndmin=2)\n",
    "            ahc_auc_df = pd.DataFrame(ahc_auc_array)  #here we have the auc data in a dataframe\n",
    "            ahc_auc_df['file_name'] = file_name\n",
    "            ahc_auc_append.append(ahc_auc_df)\n",
    "            ahc_auc_concat_df = pd.concat(ahc_auc_append)\n",
    "\n",
    "            #Block of code for decay tau\n",
    "            trace_for_kinetics = flattened_average_sk_trace[time_from_analysis_start_time_for_decay_tau1:]\n",
    "            times_for_kinetics = flat_times[time_from_analysis_start_time_for_decay_tau1:]\n",
    "            trace_for_kinetics_pd = pd.DataFrame(trace_for_kinetics)\n",
    "            #trace_for_kinetics_pd.to_excel(\"trace_for_kinetics_pd.xlsx\")\n",
    "            times_for_kinetics_pd = pd.DataFrame(times_for_kinetics)\n",
    "            #times_for_kinetics_pd.to_excel(\"times_for_kinetics_pd.xlsx\")\n",
    "\n",
    "            #fit the curve for inactivation tau\n",
    "            p0 = [1.187 * 10 ** 150., .02, +16]  #values near what we expect   #here\n",
    "            params, cv = scipy.optimize.curve_fit(monoExp, times_for_kinetics, trace_for_kinetics, p0,\n",
    "                                                  bounds=(-np.inf, np.inf),\n",
    "                                                  maxfev=100000)  #here  #this fits the training curve with an r-squared of 0.97\n",
    "            m, t, b = params  #here\n",
    "            #m, t = params\n",
    "            sampleRate = sampling_rate_hz  #hz\n",
    "            tauSec = (1 / t) / sampleRate\n",
    "\n",
    "            #determine quality of fit\n",
    "            squaredDiffs = np.square(trace_for_kinetics - monoExp(times_for_kinetics, m, t, b))  #here\n",
    "            squaredDiffsFromMean = np.square(trace_for_kinetics - np.mean(trace_for_kinetics))\n",
    "            rSquared = 1 - np.sum(squaredDiffs) / np.sum(\n",
    "                squaredDiffsFromMean)  #we want these, but they arent super important to display\n",
    "            #print(f\"R^2 = {rSquared}\")\n",
    "\n",
    "            #plot results\n",
    "            #plt.plot(times_for_kinetics, trace_for_kinetics, '.', label=\"data\")\n",
    "            #plt.plot(times_for_kinetics, monoExp(times_for_kinetics, m, t, b), '--', label=\"fitted\")  #here\n",
    "            plt.show()\n",
    "            #plt.title(\"Fitted Expotential Curve\")\n",
    "\n",
    "            #inspect the params\n",
    "            #print(f\"Y = {m} * e^(-{t} * x) + {b}\")   #the equations are important\n",
    "            #print(f\"Tau = {tauSec * 1e6} us\")    #but the tau is the most important\n",
    "            plt.show()\n",
    "            tau_flat_ms = tauSec * 1e4\n",
    "            tau_array = np.array(tauSec * 1e4, ndmin=2)\n",
    "            tau_df = pd.DataFrame(tau_array)\n",
    "            tau_df['file_name'] = file_name\n",
    "            tau_append.append(tau_df)\n",
    "            tau_concat_df = pd.concat(tau_append)\n",
    "    #for file_name_1 in os.listdir():\n",
    "    #if file_name_1.endswith(\".axgd\") or file_name_1.endswith(\".axgx\"):\n",
    "    #filename = file_name_1\n",
    "\n",
    "    #Block of code to add a column containing the file name for each metric, change the column name, and print to excel\n",
    "    #ahc_max_amp_concat_df['File'] = filename\n",
    "    ahc_max_amp_concat_df.rename(columns = {0:'Max Amplitude(pA)'}, inplace=True)\n",
    "    ahc_max_amp_concat_df.to_excel('Gen_'+'ahc_max_amp' + '.xlsx', index=False)\n",
    "    #bit of tricky code for the auc\n",
    "\n",
    "    #ahc_auc_concat_df['File'] = filename\n",
    "    ahc_auc_concat_df.rename(columns = {0:'AUC (pA*s)'}, inplace=True)\n",
    "    ahc_auc_concat_df.to_excel('Gen_'+'ahc_auc' + '.xlsx', index=False)\n",
    "\n",
    "    #same bit for the decay tau\n",
    "    #tau_concat_df['File'] = filename\n",
    "    tau_concat_df.rename(columns = {0:'Tau (ms)'}, inplace=True)\n",
    "    tau_concat_df.to_excel('Gen_'+'ahc_tau' + '.xlsx', index=False)\n",
    "    return display(ahc_max_amp_concat_df), display(ahc_auc_concat_df), display(tau_concat_df)\n",
    "\n",
    "def AHP(path, sweep):\n",
    "    ahp_array = []\n",
    "    os.chdir(path)\n",
    "    for file_name in os.listdir():\n",
    "\n",
    "        if file_name.endswith(\".axgd\") or file_name.endswith(\".axgx\"):\n",
    "            file = load_neo_file(file_name)\n",
    "            def monoExp(x, m, t, b):\n",
    "                return m * np.exp(-t * x) + b\n",
    "            for traces in file:\n",
    "                for data in traces:\n",
    "                    #this is for the first part of the A-current\n",
    "                    times1 = data['T'][25000:30000]\n",
    "                    volts1 = data['V'][25000:30000]\n",
    "                    baseline = data['V'][4500:4910]\n",
    "                    min_volts = np.min(volts1)\n",
    "                    ahp_min = min_volts - baseline\n",
    "                    max_ahp = ahp_min[sweep]\n",
    "            ahp_array.append(max_ahp)\n",
    "    ahp_df = pd.DataFrame(ahp_array)\n",
    "    ahp_df.to_excel('AHP_min' + '.xlsx', index=False)\n",
    "    #ahp_concat = pd.concat(ahp_array, ignore_index=True, axis=0)\n",
    "    return display(ahp_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cc_path =\n",
    "os.chdir(cc_path)\n",
    "analyze_feature(cc_path, feature='ISI_CV')\n",
    "analyze_feature(cc_path, feature='time_to_first_spike')\n",
    "analyze_feature(cc_path, feature='Spikecount_stimint')\n",
    "analyze_feature(cc_path, feature='mean_frequency')\n",
    "analyze_feature(cc_path, feature='steady_state_voltage_stimend')\n",
    "analyze_feature(cc_path, feature='doublet_ISI')\n",
    "analyze_feature(cc_path, feature='amp_drop_first_last')\n",
    "analyze_feature(cc_path, feature='spike_half_width')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A_current_path =\n",
    "A_current(A_current_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ahc_path =\n",
    "sk_analysis(ahc_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
